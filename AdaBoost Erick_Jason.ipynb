{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "\n",
    "BootCamp Analytics GAP\n",
    "\n",
    "    Erick Hernández López\n",
    "    Jason Latouche Jiménez\n",
    "\n",
    "Es un técnica de optimización de modelos más débiles que se encarga de generalizar la predicción para producir un modelo más fuerte.\n",
    "\n",
    "Su nombre es derivado de la terminología \"__Adaptive Boosting__\", el cual se basa en la técnica Boosting, que consiste en el uso de diferetes modelos o algoritmos de aprendizaje automático.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagen Gráfica\n",
    "![AdaBoost](AdaBoost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning \n",
    "\n",
    "El objetivo del __Ensemble Learning__ es combinar las predicciones de varios estimadores base construidos con un algoritmo de aprendizaje dado para mejorar la generalización o robustez. Tiene dos categorías:\n",
    "\n",
    "* Métodos de promediado: \n",
    "El principio de conducción es construir varios estimadores de forma independiente y luego promediar sus predicciones. En promedio, el estimador combinado es generalmente mejor que cualquiera del estimador de base única porque su varianza es reducida.\n",
    "\n",
    "**Ejemplos:** métodos de embolsado , bosques de árboles aleatorios , entre otros.\n",
    "\n",
    "* Métodos de Boosting:\n",
    "Los estimadores de base se construyen secuencialmente y uno trata de reducir el sesgo del estimador combinado. La motivación es combinar varios modelos débiles para producir un conjunto poderoso.\n",
    "\n",
    "**Ejemplos:** AdaBoost , Gradient Tree Boosting , entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de AdaBost\n",
    "\n",
    "Para este ejemplo se usa scikit-Learn y el Dataset de Iris que pertenece a la Universidad de California (UCI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías a usar\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se carga el conjunto de datos Iris\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separan los datos en datos de entrenamiento y prueba usando la función rain_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciación del modelo\n",
    "tree_model = tree.DecisionTreeClassifier(max_depth = 1,\n",
    "                                    criterion = 'entropy', \n",
    "                                    class_weight = 'balanced',\n",
    "                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6888888888888889"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "tree_model.fit(X_train, y_train) \n",
    "\n",
    "# Prueba\n",
    "tree_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros \n",
    "\n",
    "* **base_estimator:** \n",
    "\n",
    "El estimador base a partir del cual se construye la técnica. Se requiere soporte para la ponderación de la muestra, así como atributos apropiados de classes y n-classe.\n",
    "\n",
    "* **n_estimators:**\n",
    "\n",
    "El número máximo de estimadores en los que se finaliza el entrenamiento. En caso de ajuste perfecto, el proceso de aprendizaje se detiene temprano.\n",
    "\n",
    "* **learning_rate:**\n",
    "\n",
    "La tasa de aprendizaje reduce la contribución de cada clasificador learning_rate. Hay una compensación entre learning_ratey n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=tree_model,\n",
    "                         n_estimators=3,\n",
    "                         learning_rate=0.1,\n",
    "                         random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "clf.fit(X_test, y_test)\n",
    "\n",
    "# Prueba\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
